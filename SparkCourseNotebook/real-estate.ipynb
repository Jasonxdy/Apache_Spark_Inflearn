{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.ml.regression import DecisionTreeRegressor\n", "from pyspark.sql import SparkSession\n", "from pyspark.ml.feature import VectorAssembler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n\n", "    # Create a SparkSession (Note, the config section is only for Windows!)\n", "    spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()\n", "    \n", "    # Load up data as dataframe\n", "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n", "        .csv(\"file:///SparkCourse/realestate.csv\")\n", "    assembler = VectorAssembler().setInputCols([\"HouseAge\", \"DistanceToMRT\", \\\n", "                               \"NumberConvenienceStores\"]).setOutputCol(\"features\")\n", "    \n", "    df = assembler.transform(data).select(\"PriceOfUnitArea\", \"features\")\n\n", "    # Let's split our data into training data and testing data\n", "    trainTest = df.randomSplit([0.5, 0.5])\n", "    trainingDF = trainTest[0]\n", "    testDF = trainTest[1]\n\n", "    # Now create our decision tree\n", "    dtr = DecisionTreeRegressor().setFeaturesCol(\"features\").setLabelCol(\"PriceOfUnitArea\")\n\n", "    # Train the model using our training data\n", "    model = dtr.fit(trainingDF)\n\n", "    # Now see if we can predict values in our test data.\n", "    # Generate predictions using our decision tree model for all features in our\n", "    # test dataframe:\n", "    fullPredictions = model.transform(testDF).cache()\n\n", "    # Extract the predictions and the \"known\" correct labels.\n", "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n", "    labels = fullPredictions.select(\"PriceOfUnitArea\").rdd.map(lambda x: x[0])\n\n", "    # Zip them together\n", "    predictionAndLabel = predictions.zip(labels).collect()\n\n", "    # Print out the predicted and actual values for each point\n", "    for prediction in predictionAndLabel:\n", "      print(prediction)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Stop the session\n", "    spark.stop()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}