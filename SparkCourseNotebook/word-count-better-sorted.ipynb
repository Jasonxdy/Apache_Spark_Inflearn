{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "from pyspark import SparkConf, SparkContext"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalizeWords(text):\n", "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n", "sc = SparkContext(conf = conf)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["input = sc.textFile(\"file:///sparkcourse/book.txt\")\n", "words = input.flatMap(normalizeWords)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n", "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey()\n", "results = wordCountsSorted.collect()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for result in results:\n", "    count = str(result[0])\n", "    word = result[1].encode('ascii', 'ignore')\n", "    if (word):\n", "        print(word.decode() + \":\\t\\t\" + count)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}